최종 목표
Distributed LLaMa PC(x86)/라즈베리파이 통해서 실행 후 해당 기술의 장점 / 개선할 점 찾기

현재 생각하는 장점 & 단점 (0429)

장점
1) 분산 환경이기때문에, 사용자의 환경에 크게 구애 받지않음

단점
1) 출력 방식의 문제 (수정 가능성 있으며, 이미 구현되어있을 수 있으니 구성 확인 필요)
2) 처음에 명령한 steps수를 무조건 다 사용하려고 하여, 대답이 끝난 후에도 지속적으로 다른 이야기를 진행함 (위와 동일하게 확인 필요)

주간 목표(04/29~05/03)
-워크스테이션이 아닌 2개의 노트북 환경에서 추가 실행
-출력 방식의 문제 수정
-상세한 성능 비교후 시각자료 생성



진행 상황

04/29(월)
- LLaMa 3 설치
- Distributed-LLaMa with LLaMa 2 / LLaMa 3  실행

질문 Prompt : "Please explain the distributed system."

Distributed- LLaMa 2
everybody can share the same data, the same resource, and even the same task. It is a good solution for large data or application
that needs to distribute computing power. A distributed system is a system that consists of many loosely coupled computing machines
that communicate and coordinate their actions by passing messages to one another. Distributed computing is the idea that a problem
can be split into many parts, and then the results of each part can be combined to get a solution. //103 token

A distributed system is a collection of independent computers that appear to work together as a single system. 
Distributed computing is the idea

1 token 당 평균 응답 소요시간 447.05 ms , 총합 57,222ms로 약 57초 소요되었다.
추가적으로, 반복을 제외하면 103 token 사용하였으므로 약 46초 소요되었다.

Distributed-LLaMa 3 
(1). Distributed system is a collection of networked computers (i.e. hosts) that coordinate their activities to solve a problem. 
A distributed system is a network of autonomous computers linked by communication channels that enable the computers to coordinate
their activities and to share their resources so that they can function as a coordinated whole. //71 token
(이후 반복)
1 token 당 평균 응답 소요시간 387.55 ms , 총합 49,606 ms 로 약 50초 소요되었다.
추가적으로, 반복을 제외하면 71 token 사용하였으므로 약 27.5초 소요되었다. 

LLaMa2의 답변과 LLaMa 3의 답변이 모두 정확도 측면에서 좋은 내용을 보여주였으며, 틀린 내용을 기재하지않았다.
코딩과 관련된, 더 틀리기 쉬운 내용들을 물어보아 성능 비교 추가로 진행할 예정이다.

04/30 (화)
- BK 서류작성
- 자원 사용량 분석
- 프롬프트 생성후 비교
"How can performance be optimized in Java multithreading programs? Specifically, explain how to prevent race conditions and deadlocks during parallel processing."
LLaMa2 : How can performance be optimized in Java multithreading programs specifically, 
explain how to prevent race conditions and deadlocks during parallel processing. фев 2, 2017. 
The Java Concurrency API provides a simple way to create thread pools. 
This makes the writing of thread pools much easier. java.util.concurrent.ScheduledThreadPoolExecutor; 
java.util.concurrent.ExecutorService; java.util.concurrent.ThreadPoolExecutor; java.util.concurrent.ScheduledThreadPoolExecutor; 
java.util.concurrent.ExecutorService; java.util.concurrent.ScheduledThreadPoolExecutor. 
Coding Notes: Multithreading in Java. This is the first of a new series of Coding Notes, 
where I will... 26 Jan 2017. In this tutorial, we are going to learn how to use ScheduledThreadPoolExecutor in Java. 
ScheduledThreadPoolExecutor in Java: This tutorial will... 21 May 2018. 
ScheduledThreadPoolExecutor is a subclass of ThreadPoolExecutor and it's used for scheduling tasks in a specific time interval.

LLaMa3: How can performance be optimized in Java multithreading programs? 
Specifically, explain how to prevent race conditions and deadlocks during parallel processing. 
Provide an example of how race conditions and deadlocks can occur and how they can be avoided. 
Java multithreading programs can be optimized by using the volatile keyword to ensure that variables are shared across threads and that changes made to these variables are reflected in all threads. 
This prevents race conditions, which occur when multiple threads access the same variable at the same time, and the values of that variable are not properly synchronized.
Additionally, deadlocks can occur when two or more threads are waiting for each other to complete a task, and the deadlock can only be resolved by manually breaking the cycle. 
To avoid deadlocks, use a lock manager to ensure that resources are accessed sequentially, and try to avoid blocking threads by ensuring that they can continue their tasks without waiting for other threads. 
For example, in a banking system, multiple threads might be trying to access the same account balance, which could lead to race conditions if the value is not properly synchronized. 
To prevent this, the volatile keyword can be used to ensure that the value is updated immediately, and locks can be used to ensure that only one thread accesses the account balance at a time. 
This prevents race conditions and ensures that the value is always accurate.

LLaMa2-7B는 스레딩에 대한 정보를 주지만, 정작 질문 내용을 잘 해결하지 못함.
두번째의 LLaMa3-8B는 올바른 정보를 제공.

이후 추가적으로 다른 프롬프트를 2-3개 사용해보았으나, 프롬프트에 대한 답변을 제시하지 않고,
프롬프트 내용을 그대로 출력하는 에러 발생하여 해결하려고 시도중.
자원 사용량
LLaMa2 7B 사용(384토큰) : 4335184kb(4.3GB)의 램 사용.
LLaMa3 8B 사용(384토큰) : 6285436kb(6.3GB)의 램 사용.

수요일 할 것 : LLaMa3 제대로 안뜨는 에러 해결 및 노트북환경에서의 실행, 출력 방식의 변경 시도.

05/01(수)
-LLaMa3 제대로 안뜨는 에러 :
동일한 질문을 물어보더라도, 제대로된 답변이 아닌 이상한 답변을 주는 경우가 간혹 있음(답변: 000011111000 과 같이)
LLaMa3의 불안정성일수도, Distributed LLaMA의 불안정성일수도 있으므로 계속 확인해봐야할듯.
다시 물어보면 제대로된 답변을 해줌. (약 10번에 1번꼴로 에러 발생 추정) 

- 출력 방식의 변경 시도
Terminal에 직접 Type 하는 방식에서, Python 코드로 수정하는 방식으로 진행.
Python으로 수정 과정에서 RAM 사용량에 증가를 보이는지 체크하였으나, RAM 사용량은 이상이 없었음.
허나 노트북, Raspberry Pi 등에서는 다른 결과를 보일수 있으므로 다른 환경에서의 추가 확인 진행해볼 예정.

코드 초안은 아래와 같음.

'''
import subprocess

def run_inference():
    command = [
        "./main", 
        "inference",
        "--weights-float-type", "q40",
        "--buffer-float-type", "q80",
        "--prompt", "Explain about llama",
        "--steps", "256",
        "--nthreads", "8",
        "--model", "dllama_meta-llama-3-8b_q40.bin",
        "--tokenizer", "dllama-llama3-tokenizer.t"
    ]

    result = subprocess.run(command, text=True, capture_output=True)

    if result.returncode == 0:
        print("Inference successful!")
        print("Output:", result.stdout)
    else:
        print("Error in inference process")
        print("Error:", result.stderr)  # 오류 메시지 출력

run_inference()
'''

해당 방식을 통해, 기존의 Terminal을 통해 출력하던 내용들이 Python에서 정상적으로 출력되는것을 확인.

이후, 각 문장의 마지막 부분들을 뽑아내 해당 내용들을 문장으로 만드는 과정을 위해 수정작업을 거쳤다.
먼저,  Distributed LLaMa의 경우 아래와 같이 Token 별로 출력을 하기때문에, 먼저 '🔶' 가 포함된 줄의 마지막 단어를 출력하게 진행하였다.
🔶 G  387 ms I  385 ms T	1 ms S  	0 kB R  	0 kB  are
🔶 G  386 ms I  385 ms T	0 ms S  	0 kB R  	0 kB  there
🔶 G  384 ms I  381 ms T	2 ms S  	0 kB R  	0 kB  in

대부분의 경우에 잘 작동하였으나,
🔶 G  387 ms I  385 ms T	1 ms S  	0 kB R  	0 kB 
와 같이 마지막에 공백만 포함된 경우에는 kB를 출력하는 문제가 발생하였다.
그리하여, '0 kB' 를 두번 까지 파악한뒤, 그뒤의 단어를 출력하는 과정으로 수정하였다.
그렇게 수정하고나니 문제없이 문장을 합쳐서 출력해주는 것을 확인할 수 있었다.

허나, 해당과정을 통해 기존대비 실행시간이 약 10% 정도 길어진 경향을 보였다. (예시테스트의 경우, 기존 57.9s, 해당 코드 63.3s로 실행시간 약 9.3% 증가)
이후 많은 step을 사용하여 더많은 시간이 소요되는 경우 문제가 생길 수 있으니 해당 경우에 대해서도 테스트를 해보고, 동일하게 약 10%정도 지연될 경우 다른 방식을 추가로 탐색해볼 예정이다.

05/02(목)

- 많은양의 steps과 thread를 주어 답변 차이 비교
기존의 8thread에서, 최대 128 thread까지 증가시켰으며,
steps의 경우에도 기존의 128~512에서 8~32배 증가한 4096까지 확대.
4096의 경우, 답변 생성에 너무 오랜 시간이 걸려 (5시간 넘게 답변을 주지않음) 내일 와서 다시 확인해 볼 예정.
4096/128의 경우, 답변 생성하지 않았고, 그저 물어본 질문을 반복하는데만 약 10분의 시간이 걸려 응답.

- 더 낮은 환경에서의 확인위해 노트북에 Linux 세팅 작업 진행중.

05/03(금)

- 여러 Prompt를 통해 LLaMa3 체험중

현재 파악된 단점
- ChatGPT와 같은, 1번질문 후 2번질문을 이어서 하는 식으로 할 수 없음 (LLaMa 자체의 문제인지, Prompt 형식의 문제인지 확인해볼 예정)
- ChatGPT보다 정확하고, 구체적으로 물어봐야 답변해줌. (네트워크에 대해 알려달라고 하면, 네트워크의 IPv4가 무엇인지 등에 대해 질문하는데, 네트워크에 대해 공부하고 싶으니, 네트워크에 대해 알려달라고 질문시 전반적인 정보를 줌
- Fine-Tuning을 통해 개선해야할 내용들이 많다고 판단

- 답변에 따라서, 같은 답변을 반복해서 출력할 때가 있고, 답변을 1회 출력 후 더 이상 출력하지 않는 경우가 있는데, 후자로 통일하도록 개선 예정

05/07(화)
예비군 훈련

05/08(수)
- 노트북에 Linux 설치 및 환경 설정
- Distributed LLaMa 이외에 분산환경에서 LLM을 사용한 논문이 있는지 확인.
