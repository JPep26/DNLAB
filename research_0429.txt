ìµœì¢… ëª©í‘œ
Distributed LLaMa PC(x86)/ë¼ì¦ˆë² ë¦¬íŒŒì´ í†µí•´ì„œ ì‹¤í–‰ í›„ í•´ë‹¹ ê¸°ìˆ ì˜ ì¥ì  / ê°œì„ í•  ì  ì°¾ê¸°

í˜„ì¬ ìƒê°í•˜ëŠ” ì¥ì  & ë‹¨ì  (0429)

ì¥ì 
1) ë¶„ì‚° í™˜ê²½ì´ê¸°ë•Œë¬¸ì—, ì‚¬ìš©ìì˜ í™˜ê²½ì— í¬ê²Œ êµ¬ì•  ë°›ì§€ì•ŠìŒ

ë‹¨ì 
1) ì¶œë ¥ ë°©ì‹ì˜ ë¬¸ì œ (ìˆ˜ì • ê°€ëŠ¥ì„± ìˆìœ¼ë©°, ì´ë¯¸ êµ¬í˜„ë˜ì–´ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ êµ¬ì„± í™•ì¸ í•„ìš”)
2) ì²˜ìŒì— ëª…ë ¹í•œ stepsìˆ˜ë¥¼ ë¬´ì¡°ê±´ ë‹¤ ì‚¬ìš©í•˜ë ¤ê³  í•˜ì—¬, ëŒ€ë‹µì´ ëë‚œ í›„ì—ë„ ì§€ì†ì ìœ¼ë¡œ ë‹¤ë¥¸ ì´ì•¼ê¸°ë¥¼ ì§„í–‰í•¨ (ìœ„ì™€ ë™ì¼í•˜ê²Œ í™•ì¸ í•„ìš”)

ì£¼ê°„ ëª©í‘œ(04/29~05/03)
-ì›Œí¬ìŠ¤í…Œì´ì…˜ì´ ì•„ë‹Œ 2ê°œì˜ ë…¸íŠ¸ë¶ í™˜ê²½ì—ì„œ ì¶”ê°€ ì‹¤í–‰
-ì¶œë ¥ ë°©ì‹ì˜ ë¬¸ì œ ìˆ˜ì •
-ìƒì„¸í•œ ì„±ëŠ¥ ë¹„êµí›„ ì‹œê°ìë£Œ ìƒì„±



ì§„í–‰ ìƒí™©

04/29(ì›”)
- LLaMa 3 ì„¤ì¹˜
- Distributed-LLaMa with LLaMa 2 / LLaMa 3  ì‹¤í–‰

ì§ˆë¬¸ Prompt : "Please explain the distributed system."

Distributed- LLaMa 2
everybody can share the same data, the same resource, and even the same task. It is a good solution for large data or application
that needs to distribute computing power. A distributed system is a system that consists of many loosely coupled computing machines
that communicate and coordinate their actions by passing messages to one another. Distributed computing is the idea that a problem
can be split into many parts, and then the results of each part can be combined to get a solution. //103 token

A distributed system is a collection of independent computers that appear to work together as a single system. 
Distributed computing is the idea

1 token ë‹¹ í‰ê·  ì‘ë‹µ ì†Œìš”ì‹œê°„ 447.05 ms , ì´í•© 57,222msë¡œ ì•½ 57ì´ˆ ì†Œìš”ë˜ì—ˆë‹¤.
ì¶”ê°€ì ìœ¼ë¡œ, ë°˜ë³µì„ ì œì™¸í•˜ë©´ 103 token ì‚¬ìš©í•˜ì˜€ìœ¼ë¯€ë¡œ ì•½ 46ì´ˆ ì†Œìš”ë˜ì—ˆë‹¤.

Distributed-LLaMa 3 
(1). Distributed system is a collection of networked computers (i.e. hosts) that coordinate their activities to solve a problem. 
A distributed system is a network of autonomous computers linked by communication channels that enable the computers to coordinate
their activities and to share their resources so that they can function as a coordinated whole. //71 token
(ì´í›„ ë°˜ë³µ)
1 token ë‹¹ í‰ê·  ì‘ë‹µ ì†Œìš”ì‹œê°„ 387.55 ms , ì´í•© 49,606 ms ë¡œ ì•½ 50ì´ˆ ì†Œìš”ë˜ì—ˆë‹¤.
ì¶”ê°€ì ìœ¼ë¡œ, ë°˜ë³µì„ ì œì™¸í•˜ë©´ 71 token ì‚¬ìš©í•˜ì˜€ìœ¼ë¯€ë¡œ ì•½ 27.5ì´ˆ ì†Œìš”ë˜ì—ˆë‹¤. 

LLaMa2ì˜ ë‹µë³€ê³¼ LLaMa 3ì˜ ë‹µë³€ì´ ëª¨ë‘ ì •í™•ë„ ì¸¡ë©´ì—ì„œ ì¢‹ì€ ë‚´ìš©ì„ ë³´ì—¬ì£¼ì˜€ìœ¼ë©°, í‹€ë¦° ë‚´ìš©ì„ ê¸°ì¬í•˜ì§€ì•Šì•˜ë‹¤.
ì½”ë”©ê³¼ ê´€ë ¨ëœ, ë” í‹€ë¦¬ê¸° ì‰¬ìš´ ë‚´ìš©ë“¤ì„ ë¬¼ì–´ë³´ì•„ ì„±ëŠ¥ ë¹„êµ ì¶”ê°€ë¡œ ì§„í–‰í•  ì˜ˆì •ì´ë‹¤.

04/30 (í™”)
- BK ì„œë¥˜ì‘ì„±
- ìì› ì‚¬ìš©ëŸ‰ ë¶„ì„
- í”„ë¡¬í”„íŠ¸ ìƒì„±í›„ ë¹„êµ
"How can performance be optimized in Java multithreading programs? Specifically, explain how to prevent race conditions and deadlocks during parallel processing."
LLaMa2 : How can performance be optimized in Java multithreading programs specifically, 
explain how to prevent race conditions and deadlocks during parallel processing. Ñ„ĞµĞ² 2, 2017. 
The Java Concurrency API provides a simple way to create thread pools. 
This makes the writing of thread pools much easier. java.util.concurrent.ScheduledThreadPoolExecutor; 
java.util.concurrent.ExecutorService; java.util.concurrent.ThreadPoolExecutor; java.util.concurrent.ScheduledThreadPoolExecutor; 
java.util.concurrent.ExecutorService; java.util.concurrent.ScheduledThreadPoolExecutor. 
Coding Notes: Multithreading in Java. This is the first of a new series of Coding Notes, 
where I will... 26 Jan 2017. In this tutorial, we are going to learn how to use ScheduledThreadPoolExecutor in Java. 
ScheduledThreadPoolExecutor in Java: This tutorial will... 21 May 2018. 
ScheduledThreadPoolExecutor is a subclass of ThreadPoolExecutor and it's used for scheduling tasks in a specific time interval.

LLaMa3: How can performance be optimized in Java multithreading programs? 
Specifically, explain how to prevent race conditions and deadlocks during parallel processing. 
Provide an example of how race conditions and deadlocks can occur and how they can be avoided. 
Java multithreading programs can be optimized by using the volatile keyword to ensure that variables are shared across threads and that changes made to these variables are reflected in all threads. 
This prevents race conditions, which occur when multiple threads access the same variable at the same time, and the values of that variable are not properly synchronized.
Additionally, deadlocks can occur when two or more threads are waiting for each other to complete a task, and the deadlock can only be resolved by manually breaking the cycle. 
To avoid deadlocks, use a lock manager to ensure that resources are accessed sequentially, and try to avoid blocking threads by ensuring that they can continue their tasks without waiting for other threads. 
For example, in a banking system, multiple threads might be trying to access the same account balance, which could lead to race conditions if the value is not properly synchronized. 
To prevent this, the volatile keyword can be used to ensure that the value is updated immediately, and locks can be used to ensure that only one thread accesses the account balance at a time. 
This prevents race conditions and ensures that the value is always accurate.

LLaMa2-7BëŠ” ìŠ¤ë ˆë”©ì— ëŒ€í•œ ì •ë³´ë¥¼ ì£¼ì§€ë§Œ, ì •ì‘ ì§ˆë¬¸ ë‚´ìš©ì„ ì˜ í•´ê²°í•˜ì§€ ëª»í•¨.
ë‘ë²ˆì§¸ì˜ LLaMa3-8BëŠ” ì˜¬ë°”ë¥¸ ì •ë³´ë¥¼ ì œê³µ.

ì´í›„ ì¶”ê°€ì ìœ¼ë¡œ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ë¥¼ 2-3ê°œ ì‚¬ìš©í•´ë³´ì•˜ìœ¼ë‚˜, í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œì‹œí•˜ì§€ ì•Šê³ ,
í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ëŠ” ì—ëŸ¬ ë°œìƒí•˜ì—¬ í•´ê²°í•˜ë ¤ê³  ì‹œë„ì¤‘.
ìì› ì‚¬ìš©ëŸ‰
LLaMa2 7B ì‚¬ìš©(384í† í°) : 4335184kb(4.3GB)ì˜ ë¨ ì‚¬ìš©.
LLaMa3 8B ì‚¬ìš©(384í† í°) : 6285436kb(6.3GB)ì˜ ë¨ ì‚¬ìš©.

ìˆ˜ìš”ì¼ í•  ê²ƒ : LLaMa3 ì œëŒ€ë¡œ ì•ˆëœ¨ëŠ” ì—ëŸ¬ í•´ê²° ë° ë…¸íŠ¸ë¶í™˜ê²½ì—ì„œì˜ ì‹¤í–‰, ì¶œë ¥ ë°©ì‹ì˜ ë³€ê²½ ì‹œë„.

05/01(ìˆ˜)
-LLaMa3 ì œëŒ€ë¡œ ì•ˆëœ¨ëŠ” ì—ëŸ¬ :
ë™ì¼í•œ ì§ˆë¬¸ì„ ë¬¼ì–´ë³´ë”ë¼ë„, ì œëŒ€ë¡œëœ ë‹µë³€ì´ ì•„ë‹Œ ì´ìƒí•œ ë‹µë³€ì„ ì£¼ëŠ” ê²½ìš°ê°€ ê°„í˜¹ ìˆìŒ(ë‹µë³€: 000011111000 ê³¼ ê°™ì´)
LLaMa3ì˜ ë¶ˆì•ˆì •ì„±ì¼ìˆ˜ë„, Distributed LLaMAì˜ ë¶ˆì•ˆì •ì„±ì¼ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ ê³„ì† í™•ì¸í•´ë´ì•¼í• ë“¯.
ë‹¤ì‹œ ë¬¼ì–´ë³´ë©´ ì œëŒ€ë¡œëœ ë‹µë³€ì„ í•´ì¤Œ. (ì•½ 10ë²ˆì— 1ë²ˆê¼´ë¡œ ì—ëŸ¬ ë°œìƒ ì¶”ì •) 

- ì¶œë ¥ ë°©ì‹ì˜ ë³€ê²½ ì‹œë„
Terminalì— ì§ì ‘ Type í•˜ëŠ” ë°©ì‹ì—ì„œ, Python ì½”ë“œë¡œ ìˆ˜ì •í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì§„í–‰.
Pythonìœ¼ë¡œ ìˆ˜ì • ê³¼ì •ì—ì„œ RAM ì‚¬ìš©ëŸ‰ì— ì¦ê°€ë¥¼ ë³´ì´ëŠ”ì§€ ì²´í¬í•˜ì˜€ìœ¼ë‚˜, RAM ì‚¬ìš©ëŸ‰ì€ ì´ìƒì´ ì—†ì—ˆìŒ.
í—ˆë‚˜ ë…¸íŠ¸ë¶, Raspberry Pi ë“±ì—ì„œëŠ” ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ë³´ì¼ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¤ë¥¸ í™˜ê²½ì—ì„œì˜ ì¶”ê°€ í™•ì¸ ì§„í–‰í•´ë³¼ ì˜ˆì •.

ì½”ë“œ ì´ˆì•ˆì€ ì•„ë˜ì™€ ê°™ìŒ.

'''
import subprocess

def run_inference():
    command = [
        "./main", 
        "inference",
        "--weights-float-type", "q40",
        "--buffer-float-type", "q80",
        "--prompt", "Explain about llama",
        "--steps", "256",
        "--nthreads", "8",
        "--model", "dllama_meta-llama-3-8b_q40.bin",
        "--tokenizer", "dllama-llama3-tokenizer.t"
    ]

    result = subprocess.run(command, text=True, capture_output=True)

    if result.returncode == 0:
        print("Inference successful!")
        print("Output:", result.stdout)
    else:
        print("Error in inference process")
        print("Error:", result.stderr)  # ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥

run_inference()
'''

í•´ë‹¹ ë°©ì‹ì„ í†µí•´, ê¸°ì¡´ì˜ Terminalì„ í†µí•´ ì¶œë ¥í•˜ë˜ ë‚´ìš©ë“¤ì´ Pythonì—ì„œ ì •ìƒì ìœ¼ë¡œ ì¶œë ¥ë˜ëŠ”ê²ƒì„ í™•ì¸.

ì´í›„, ê° ë¬¸ì¥ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ë“¤ì„ ë½‘ì•„ë‚´ í•´ë‹¹ ë‚´ìš©ë“¤ì„ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“œëŠ” ê³¼ì •ì„ ìœ„í•´ ìˆ˜ì •ì‘ì—…ì„ ê±°ì³¤ë‹¤.
ë¨¼ì €,  Distributed LLaMaì˜ ê²½ìš° ì•„ë˜ì™€ ê°™ì´ Token ë³„ë¡œ ì¶œë ¥ì„ í•˜ê¸°ë•Œë¬¸ì—, ë¨¼ì € 'ğŸ”¶' ê°€ í¬í•¨ëœ ì¤„ì˜ ë§ˆì§€ë§‰ ë‹¨ì–´ë¥¼ ì¶œë ¥í•˜ê²Œ ì§„í–‰í•˜ì˜€ë‹¤.
ğŸ”¶ G  387 ms I  385 ms T	1 ms S  	0 kB R  	0 kB  are
ğŸ”¶ G  386 ms I  385 ms T	0 ms S  	0 kB R  	0 kB  there
ğŸ”¶ G  384 ms I  381 ms T	2 ms S  	0 kB R  	0 kB  in

ëŒ€ë¶€ë¶„ì˜ ê²½ìš°ì— ì˜ ì‘ë™í•˜ì˜€ìœ¼ë‚˜,
ğŸ”¶ G  387 ms I  385 ms T	1 ms S  	0 kB R  	0 kB 
ì™€ ê°™ì´ ë§ˆì§€ë§‰ì— ê³µë°±ë§Œ í¬í•¨ëœ ê²½ìš°ì—ëŠ” kBë¥¼ ì¶œë ¥í•˜ëŠ” ë¬¸ì œê°€ ë°œìƒí•˜ì˜€ë‹¤.
ê·¸ë¦¬í•˜ì—¬, '0 kB' ë¥¼ ë‘ë²ˆ ê¹Œì§€ íŒŒì•…í•œë’¤, ê·¸ë’¤ì˜ ë‹¨ì–´ë¥¼ ì¶œë ¥í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ ìˆ˜ì •í•˜ì˜€ë‹¤.
ê·¸ë ‡ê²Œ ìˆ˜ì •í•˜ê³ ë‚˜ë‹ˆ ë¬¸ì œì—†ì´ ë¬¸ì¥ì„ í•©ì³ì„œ ì¶œë ¥í•´ì£¼ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆë‹¤.

í—ˆë‚˜, í•´ë‹¹ê³¼ì •ì„ í†µí•´ ê¸°ì¡´ëŒ€ë¹„ ì‹¤í–‰ì‹œê°„ì´ ì•½ 10% ì •ë„ ê¸¸ì–´ì§„ ê²½í–¥ì„ ë³´ì˜€ë‹¤. (ì˜ˆì‹œí…ŒìŠ¤íŠ¸ì˜ ê²½ìš°, ê¸°ì¡´ 57.9s, í•´ë‹¹ ì½”ë“œ 63.3së¡œ ì‹¤í–‰ì‹œê°„ ì•½ 9.3% ì¦ê°€)
ì´í›„ ë§ì€ stepì„ ì‚¬ìš©í•˜ì—¬ ë”ë§ì€ ì‹œê°„ì´ ì†Œìš”ë˜ëŠ” ê²½ìš° ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆìœ¼ë‹ˆ í•´ë‹¹ ê²½ìš°ì— ëŒ€í•´ì„œë„ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë³´ê³ , ë™ì¼í•˜ê²Œ ì•½ 10%ì •ë„ ì§€ì—°ë  ê²½ìš° ë‹¤ë¥¸ ë°©ì‹ì„ ì¶”ê°€ë¡œ íƒìƒ‰í•´ë³¼ ì˜ˆì •ì´ë‹¤.

05/02(ëª©)

- ë§ì€ì–‘ì˜ stepsê³¼ threadë¥¼ ì£¼ì–´ ë‹µë³€ ì°¨ì´ ë¹„êµ
ê¸°ì¡´ì˜ 8threadì—ì„œ, ìµœëŒ€ 128 threadê¹Œì§€ ì¦ê°€ì‹œì¼°ìœ¼ë©°,
stepsì˜ ê²½ìš°ì—ë„ ê¸°ì¡´ì˜ 128~512ì—ì„œ 8~32ë°° ì¦ê°€í•œ 4096ê¹Œì§€ í™•ëŒ€.
4096ì˜ ê²½ìš°, ë‹µë³€ ìƒì„±ì— ë„ˆë¬´ ì˜¤ëœ ì‹œê°„ì´ ê±¸ë ¤ (5ì‹œê°„ ë„˜ê²Œ ë‹µë³€ì„ ì£¼ì§€ì•ŠìŒ) ë‚´ì¼ ì™€ì„œ ë‹¤ì‹œ í™•ì¸í•´ ë³¼ ì˜ˆì •.
4096/128ì˜ ê²½ìš°, ë‹µë³€ ìƒì„±í•˜ì§€ ì•Šì•˜ê³ , ê·¸ì € ë¬¼ì–´ë³¸ ì§ˆë¬¸ì„ ë°˜ë³µí•˜ëŠ”ë°ë§Œ ì•½ 10ë¶„ì˜ ì‹œê°„ì´ ê±¸ë ¤ ì‘ë‹µ.

- ë” ë‚®ì€ í™˜ê²½ì—ì„œì˜ í™•ì¸ìœ„í•´ ë…¸íŠ¸ë¶ì— Linux ì„¸íŒ… ì‘ì—… ì§„í–‰ì¤‘.

05/03(ê¸ˆ)

- ì—¬ëŸ¬ Promptë¥¼ í†µí•´ LLaMa3 ì²´í—˜ì¤‘

í˜„ì¬ íŒŒì•…ëœ ë‹¨ì 
- ChatGPTì™€ ê°™ì€, 1ë²ˆì§ˆë¬¸ í›„ 2ë²ˆì§ˆë¬¸ì„ ì´ì–´ì„œ í•˜ëŠ” ì‹ìœ¼ë¡œ í•  ìˆ˜ ì—†ìŒ (LLaMa ìì²´ì˜ ë¬¸ì œì¸ì§€, Prompt í˜•ì‹ì˜ ë¬¸ì œì¸ì§€ í™•ì¸í•´ë³¼ ì˜ˆì •)
- ChatGPTë³´ë‹¤ ì •í™•í•˜ê³ , êµ¬ì²´ì ìœ¼ë¡œ ë¬¼ì–´ë´ì•¼ ë‹µë³€í•´ì¤Œ. (ë„¤íŠ¸ì›Œí¬ì— ëŒ€í•´ ì•Œë ¤ë‹¬ë¼ê³  í•˜ë©´, ë„¤íŠ¸ì›Œí¬ì˜ IPv4ê°€ ë¬´ì—‡ì¸ì§€ ë“±ì— ëŒ€í•´ ì§ˆë¬¸í•˜ëŠ”ë°, ë„¤íŠ¸ì›Œí¬ì— ëŒ€í•´ ê³µë¶€í•˜ê³  ì‹¶ìœ¼ë‹ˆ, ë„¤íŠ¸ì›Œí¬ì— ëŒ€í•´ ì•Œë ¤ë‹¬ë¼ê³  ì§ˆë¬¸ì‹œ ì „ë°˜ì ì¸ ì •ë³´ë¥¼ ì¤Œ
- Fine-Tuningì„ í†µí•´ ê°œì„ í•´ì•¼í•  ë‚´ìš©ë“¤ì´ ë§ë‹¤ê³  íŒë‹¨

- ë‹µë³€ì— ë”°ë¼ì„œ, ê°™ì€ ë‹µë³€ì„ ë°˜ë³µí•´ì„œ ì¶œë ¥í•  ë•Œê°€ ìˆê³ , ë‹µë³€ì„ 1íšŒ ì¶œë ¥ í›„ ë” ì´ìƒ ì¶œë ¥í•˜ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ìˆëŠ”ë°, í›„ìë¡œ í†µì¼í•˜ë„ë¡ ê°œì„  ì˜ˆì •

05/07(í™”)
ì˜ˆë¹„êµ° í›ˆë ¨

05/08(ìˆ˜)
- ë…¸íŠ¸ë¶ì— Linux ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •
- Distributed LLaMa ì´ì™¸ì— ë¶„ì‚°í™˜ê²½ì—ì„œ LLMì„ ì‚¬ìš©í•œ ë…¼ë¬¸ì´ ìˆëŠ”ì§€ í™•ì¸.
